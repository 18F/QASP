## List of Deliverables with Quality Assurance Surveillance Plan (QASP)

The government uses the QASP to monitor the quality of a vendor’s deliverables and performance. This helps the government measure vendor performance and ensure it reaches required levels as specified in the contract. The QASP provides the government with a proactive way to identify and avoid deficient performance.

The following chart sets forth the performance standards; quality levels the code and documentation provided by the Contractor must meet; and the methods [the agency] will use to assess the standard and quality levels of that code and documentation.

| **Deliverable**              | **Tested Code**                                                                               |
| ---------------------------- | --------------------------------------------------------------------------------------------- |
| **Performance Standard(s)**  | Code delivered under the order must have substantial automated test code coverage.            |
| **Acceptable Quality Level** | Minimum of 90% of code covered by automated tests. All areas of code are meaningfully tested. |
| **Method of Assessment**     | Combination of manual review and execution of the automated tests                             |

| **Deliverable**              | **Properly Styled Code**                                                                                                                       |
| ---------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------- |
| **Performance Standard(s)**  | [Link to an agency-specific coding style guide, like some of the appropriate [TTS Engineering Practices Guides](https://engineering.18f.gov/)] |
| **Acceptable Quality Level** | 0 linting errors and 0 warnings                                                                                                                |
| **Method of Assessment**     | Combination of manual review and automated testing                                                                                             |

| **Deliverable**              | **Accessible User Interface**                                                                                                                                                                                             |
| ---------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Performance Standard(s)**  | Web Content Accessibility Guidelines (WCAG) 2.1 AA standards, [GSA 18F Accessibility Guide](https://accessibility.18f.gov/), and [Digital.gov Accessibility Guide](https://accessibility.digital.gov/)<sup>1</sup>.       |
| **Acceptable Quality Level** | 0 errors reported using an automated scanner and 0 errors reported in manual testing. Required agency-specific documentation of the accessibility status is completed.                                                    |
| **Method of Assessment**     | Combination of automated and manual testing<sup>2</sup> with tools equivalent to [Accessibility Insights](https://accessibilityinsights.io/) and/or the [DHS Trusted Tester process](https://www.dhs.gov/trusted-tester). |

| **Deliverable**              | **Deployed code**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| ---------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Performance Standard(s)**  | Code must successfully build and deploy into staging and production environments<sup>3</sup>.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| **Acceptable Quality Level** | Deployment from version control of a successful build should be automated using industry standard tooling. In this project, we expect **[Government must select one of the following scenarios]**:<br/><br/>☐ To be able to trigger builds manually.<br/>☐ For builds to trigger automatically from version control (e.g. on merge).<br/>☐ Build and deployment to a non-production (staging) environment.<br/>☐ Build and deployment to a production environment.<br/>☐ Automatic rollback to the most recent working version with no downtime following an unsuccessful deployment.<br/><br/>Successful automated build through an automated deployment |
| **Method of Assessment**     | Combination of manual review and automated testing                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |

| **Deliverable**              | **Documentation**                                                                                                                                                                                                                                                                                                                                                                                                       |
| ---------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Performance Standard(s)**  | Summary of user stories completed every sprint. All dependencies are listed and the licenses are documented.<br/><br/>All dependencies are listed. Major functionality in the software/source code is documented. Individual methods are documented inline in a format that permits the use of tools such as JSDoc. System diagram is provided.<br/><br/>Relevant security controls are documented and kept up to date. |
| **Acceptable Quality Level** | Combination of manual review and automated testing, if available                                                                                                                                                                                                                                                                                                                                                        |
| **Method of Assessment**     | Manual review                                                                                                                                                                                                                                                                                                                                                                                                           |

| **Deliverable**              | **Secure code**                                                                                                                                                                                                                                                             |
| ---------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Performance Standard(s)**  | Code is free of known static and runtime vulnerabilities                                                                                                                                                                                                                    |
| **Acceptable Quality Level** | Code submitted must be free of medium- and high-level static and dynamic security vulnerabilities                                                                                                                                                                           |
| **Method of Assessment**     | Tests free of medium- and high-level vulnerabilities from a static testing SaaS (such as Snyk or npm audit), from dynamic testing tools like OWASP ZAP (with documentation explaining any false positives), and ongoing code review informed by OWASP or similar standards. |

| **Deliverable**              | **User research**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
| ---------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **Performance Standard(s)**  | Usability testing and other user [research methods](https://methods.18f.gov/) must be conducted at regular intervals throughout the development process and not just at the beginning or end of the process. The following resources should be referenced: [18F UX Guide](https://ux-guide.18f.gov/research/), [Informed Consent Guide](https://ux-guide.18f.gov/research/do/#getting-informed-consent), and [Usability Test Heuristics](https://docs.google.com/document/d/1qfGp3H1pdOlNbMYuJNQGyBIkpOcQErduDAl0adv1X-w/edit#heading=h.y2rdboc1uj3o). |
| **Acceptable Quality Level** | Research plans and artifacts from usability testing and/or other research methods with end users are available at the end of every applicable sprint, in accordance with the vendor’s research plan.<br/>Acceptable documentation for this standard can be found [here](https://github.com/18F/ux-guide/blob/master/_pages/resources/research-plan.md).                                                                                                                                                                                                |
| **Method of Assessment**     | [Agency] will manually evaluate the artifacts based on a research plan provided by the vendor at the end of the second sprint and every applicable sprint thereafter.                                                                                                                                                                                                                                                                                                                                                                                  |

| **Deliverable**              | **Accepted Features**                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| ---------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Performance Standard(s)**  | At the beginning of each sprint, the Product Owner and development team will collaborate to define a set of user stories grounded in user research to be completed during the sprint. Acceptance criteria for each story will also be defined. The development team will deliver code and functionality to satisfy these user stories.<br/><br/>Code is continuously available in a version-controlled [repository] that is owned by the Agency and in the public domain. |
| **Acceptable Quality Level** | Delivered code meets the acceptance criteria for each user story. Incomplete stories will be assessed and considered for inclusion in the next sprint.                                                                                                                                                                                                                                                                                                                    |
| **Method of Assessment**     | Manual review                                                                                                                                                                                                                                                                                                                                                                                                                                                             |

| **Deliverable**              | **API-driven Architecture**                                                                                                                                                                                    |
| ---------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Performance Standard(s)**  | Any public or private APIs developed as part of the project will be well-documented and semantically versioned.                                                                                                |
| **Acceptable Quality Level** | All API endpoints, parameters, defaults, and outputs documented and updated at the end of every applicable sprint.<br/><br/>Semantic versioning will be kept up to date at the end of every applicable sprint. |
| **Method of Assessment**     | Combination of manual review and automated testing, if available.                                                                                                                                              |

---

<sup>1</sup> Accessibility Guild is working on incorporating Accessibility for Teams into the 18F guide. Until then, we can add https://accessibility.digital.gov/ dated 10/22/2020

<sup>2</sup> Consider adding to the RFQ that users with disabilities be included in user research.

<sup>3</sup> Verify which environment(s) the vendor will be responsible for deploying code. Some agencies only allow vendors to deploy to non-production environments.
